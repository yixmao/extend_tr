{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c2eb96-b62f-416b-b2bf-3c1727016806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from extendtr.TR.topoReg import TopoReg\n",
    "from extendtr.utils.args import TopoRegArgs\n",
    "from extendtr.utils.utils import set_seed, metric_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e00d16",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6abcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the descriptor - the indices of desc will be used later\n",
    "desc = pd.read_parquet(f'./SampleDatasets/CHEMBL278/data_ECFP4.parquet', engine='fastparquet').astype('bool')\n",
    "# load targets\n",
    "data = pd.read_csv(f'./SampleDatasets/CHEMBL278/data_cp.csv', index_col=0)\n",
    "target = data[\"pChEMBL Value\"]\n",
    "# make sure that the indices of desc and target match\n",
    "desc = desc.loc[target.index]\n",
    "target = target.loc[desc.index]\n",
    "\n",
    "# load indices for scaffold split\n",
    "with open(f'./SampleDatasets/CHEMBL278/scaffold_split_index.json', 'r') as f:\n",
    "    index = json.load(f)  \n",
    "train_idx = index['train_idx']\n",
    "test_idx = index['test_idx']\n",
    "# make sure that train and test indices are included in target.index\n",
    "train_idx = [idx for idx in train_idx if idx in target.index]\n",
    "test_idx = [idx for idx in test_idx if idx in target.index]\n",
    "\n",
    "##### alternatively, you can randomly split train and test idx\n",
    "# dataset_idx = target.index.tolist()\n",
    "# train_idx, test_idx = train_test_split(dataset_idx, test_size=0.2, random_state=args.seed)\n",
    "\n",
    "# set validation index if necessary\n",
    "val_set = 0.2 # a fraction number to use [val_set] percent samples from the train set as val set, or None for no validation\n",
    "if val_set is not None: # if we want to test on the validation set\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=val_set, random_state=2021)\n",
    "else: # no validation\n",
    "    val_idx = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a248f8e",
   "metadata": {},
   "source": [
    "## Run ensemble TR with ECFP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3379ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman: 0.8288945395492194\n",
      "R2: 0.8700309651617721\n",
      "RMSE: 0.6685660816936404\n",
      "NRMSE: 0.36051218403575186\n"
     ]
    }
   ],
   "source": [
    "# get the args\n",
    "args = TopoRegArgs('-ensemble 1') # ensemble TR\n",
    "# set random seed\n",
    "set_seed(args.seed)\n",
    "\n",
    "# train and get the predictions\n",
    "mdl, pred_test, pred_val, train_time, test_time = TopoReg(desc, target, train_idx, test_idx, val_idx, args)\n",
    "\n",
    "# evaluate the resuls\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe130fb",
   "metadata": {},
   "source": [
    "## Run ensemble TR with ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c54d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman: 0.7776435139782599\n",
      "R2: 0.5956033004081223\n",
      "RMSE: 1.179310578285253\n",
      "NRMSE: 0.6359219288496644\n"
     ]
    }
   ],
   "source": [
    "# get the args\n",
    "args = TopoRegArgs('-ensemble 1 -distance jaccard -model ANN -ann_cp_dir ./results/ann_cp/') # ensemble TR, Jaccard distance, ANN model, 0.2 validation set, dir to save the checkpoints\n",
    "# set random seed\n",
    "set_seed(args.seed)\n",
    "# train and get the predictions\n",
    "mdl, pred_test, pred_val, train_time, test_time = TopoReg(desc, target, train_idx, test_idx, val_idx, args)\n",
    "# evaluate the resuls\n",
    "args.verbose = 1 # to report the metrics\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1a3ef",
   "metadata": {},
   "source": [
    "## Run Mordred descriptor with Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cedf591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman: 0.8056903350584307\n",
      "R2: 0.8766784401382622\n",
      "RMSE: 0.6512442435883308\n",
      "NRMSE: 0.35117169570131607\n"
     ]
    }
   ],
   "source": [
    "desc = pd.read_parquet(f'./SampleDatasets/CHEMBL278/data_Mordred.parquet', engine='fastparquet')\n",
    "desc = desc.loc[target.index]\n",
    "# get the args\n",
    "args = TopoRegArgs('-ensemble 1 -distance euclidean -desc_norm 1') # ensemble TR, euclidean distance, normalize the descriptors before calculate the distances\n",
    "# set random seed\n",
    "set_seed(args.seed)\n",
    "\n",
    "# train and get the predictions\n",
    "mdl, pred_test, pred_val, train_time, test_time = TopoReg(desc, target, train_idx, test_idx, val_idx, args)\n",
    "\n",
    "# evaluate the resuls\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af93a1",
   "metadata": {},
   "source": [
    "## Combine predictions from various configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5154039",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b66cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the descriptor - the indices of desc will be used later\n",
    "desc = pd.read_parquet(f'./SampleDatasets/CHEMBL278/data_ECFP4.parquet', engine='fastparquet').astype('bool')\n",
    "# load targets\n",
    "data = pd.read_csv(f'./SampleDatasets/CHEMBL278/data_cp.csv', index_col=0)\n",
    "target = data[\"pChEMBL Value\"]\n",
    "# make sure that the indices of desc and target match\n",
    "desc = desc.loc[target.index]\n",
    "target = target.loc[desc.index]\n",
    "\n",
    "# load indices for scaffold split\n",
    "with open(f'./SampleDatasets/CHEMBL278/scaffold_split_index.json', 'r') as f:\n",
    "    index = json.load(f)  \n",
    "train_idx = index['train_idx']\n",
    "test_idx = index['test_idx']\n",
    "# make sure that train and test indices are included in target.index\n",
    "train_idx = [idx for idx in train_idx if idx in target.index]\n",
    "test_idx = [idx for idx in test_idx if idx in target.index]\n",
    "\n",
    "##### alternatively, you can randomly split train and test idx\n",
    "# dataset_idx = target.index.tolist()\n",
    "# train_idx, test_idx = train_test_split(dataset_idx, test_size=0.2, random_state=args.seed)\n",
    "\n",
    "# set validation index if necessary\n",
    "val_set = 0.2 # a fraction number to use [val_set] percent samples from the train set as val set, or None for no validation\n",
    "if val_set is not None: # if we want to test on the validation set\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=val_set, random_state=2021)\n",
    "else: # no validation\n",
    "    val_idx = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3160b9a",
   "metadata": {},
   "source": [
    "### Different descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b0400a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of ensemble predictions:\n",
      "Spearman: 0.8079098676618973\n",
      "R2: 0.8286059578308224\n",
      "RMSE: 0.7677542929995983\n",
      "NRMSE: 0.4139976354632688\n",
      "Performance of stacking predictions:\n",
      "Spearman: 0.7691689349468414\n",
      "R2: 0.8473305802385793\n",
      "RMSE: 0.7246034882029492\n",
      "NRMSE: 0.39072934335857173\n"
     ]
    }
   ],
   "source": [
    "from extendtr.utils.utils import stack_models\n",
    "# define arg strs for different anchor selection methods\n",
    "args = TopoRegArgs(f'-ensemble 1')\n",
    "descriptors = ['ECFP4', 'ECFP6', 'Mordred', 'RDKdesc', 'tcnn_zpad']\n",
    "data_path = './SampleDatasets/CHEMBL278/'\n",
    "\n",
    "########################### Train and get the prediction ###############################\n",
    "preds_test = []\n",
    "preds_val = []\n",
    "for descriptor in descriptors:\n",
    "    # Load descriptors\n",
    "    if descriptor in ['ECFP4', 'ECFP6']:\n",
    "        desc = pd.read_parquet(f'{data_path}/data_{descriptor}.parquet', engine='fastparquet').astype('bool')\n",
    "    elif descriptor in ['Mordred', 'RDKdesc']:\n",
    "        desc = pd.read_parquet(f'{data_path}/data_{descriptor}.parquet', engine='fastparquet')\n",
    "    elif descriptor == 'tcnn_zpad':\n",
    "        desc = pd.read_csv(f\"{data_path}/tcnn_embeddings_zpad.csv\", index_col=0)\n",
    "    \n",
    "    if descriptor in ['ECFP4', 'ECFP6']:\n",
    "        args.distance = 'jaccard'\n",
    "        args.desc_norm = False\n",
    "    else:\n",
    "        args.distance = 'euclidean'\n",
    "        args.desc_norm = True\n",
    "\n",
    "    # set random seed\n",
    "    set_seed(args.seed)\n",
    "    mdl, pred_test, pred_val, train_time, test_time = TopoReg(desc, target, train_idx, test_idx, val_idx, args)\n",
    "    # Stack the predicted responses for test and validation sets\n",
    "    preds_test.append(pred_test)\n",
    "    preds_val.append(pred_val)\n",
    "\n",
    "########################### Combine the results ###############################\n",
    "# ensemble\n",
    "pred_test = np.array(preds_test).mean(axis=0)\n",
    "# evaluation\n",
    "print('Performance of ensemble predictions:')\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)\n",
    "\n",
    "# stacking the results\n",
    "pred_test, train_time, test_time = stack_models(preds_val, preds_test, target, val_idx)\n",
    "# evaluation\n",
    "print('Performance of stacking predictions:')\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0ee9b",
   "metadata": {},
   "source": [
    "### Different anchor selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20b4cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of ensemble predictions:\n",
      "Spearman: 0.8117436057951581\n",
      "R2: 0.8587051812567446\n",
      "RMSE: 0.6970878467718723\n",
      "NRMSE: 0.37589203069931587\n",
      "Performance of stacking predictions:\n",
      "Spearman: 0.8407993053314502\n",
      "R2: 0.8689782680175232\n",
      "RMSE: 0.6712681800377205\n",
      "NRMSE: 0.36196924176299394\n"
     ]
    }
   ],
   "source": [
    "from extendtr.utils.utils import stack_models\n",
    "# load the descriptor - the indices of desc will be used later\n",
    "desc = pd.read_parquet(f'./SampleDatasets/CHEMBL278/data_ECFP4.parquet', engine='fastparquet').astype('bool')\n",
    "# define arg strs for different anchor selection methods\n",
    "arg_strs_anchor = [f'-ensemble 1', \n",
    "                f'-anchorselection maximin -ensemble 1 -mean_anchor_percentage 0.4 -min_anchor_percentage 0.2 -max_anchor_percentage 0.6',\n",
    "                f'-refine_anchors_lasso 1 -anchor_percentage 0.8', \n",
    "                f'-anchorselection maximin_density -weight_density 0.5 -check_duplicates 1'\n",
    "                ]\n",
    "\n",
    "\n",
    "########################### Train and get the prediction ###############################\n",
    "preds_test = []\n",
    "preds_val = []\n",
    "for arg_str in arg_strs_anchor:\n",
    "    args = TopoRegArgs(f'{arg_str}')\n",
    "    # set random seed\n",
    "    set_seed(args.seed)\n",
    "    mdl, pred_test, pred_val, train_time, test_time = TopoReg(desc, target, train_idx, test_idx, val_idx, args)\n",
    "    # Stack the predicted responses for test and validation sets\n",
    "    preds_test.append(pred_test)\n",
    "    preds_val.append(pred_val)\n",
    "\n",
    "########################### Combine the results ###############################\n",
    "# ensemble\n",
    "pred_test = np.array(preds_test).mean(axis=0)\n",
    "# evaluation\n",
    "print('Performance of ensemble predictions:')\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)\n",
    "\n",
    "\n",
    "# stacking the results\n",
    "pred_test, train_time, test_time = stack_models(preds_val, preds_test, target, val_idx)\n",
    "# evaluation\n",
    "print('Performance of stacking predictions:')\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad777949",
   "metadata": {},
   "source": [
    "### Different distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b504cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of ensemble predictions:\n",
      "Spearman: 0.8250608014159587\n",
      "R2: 0.8581550874070674\n",
      "RMSE: 0.6984434917870883\n",
      "NRMSE: 0.3766230377883603\n",
      "Performance of stacking predictions:\n",
      "Spearman: 0.8627928556748934\n",
      "R2: 0.9018851154434258\n",
      "RMSE: 0.5808872463801671\n",
      "NRMSE: 0.31323295573195087\n"
     ]
    }
   ],
   "source": [
    "from extendtr.utils.utils import stack_models\n",
    "# load the descriptor - the indices of desc will be used later\n",
    "desc = pd.read_parquet(f'./SampleDatasets/CHEMBL278/data_ECFP4.parquet', engine='fastparquet').astype('bool')\n",
    "# define arg strs for different anchor selection methods\n",
    "arg_strs_dist = [f'-ensemble 1', \n",
    "                f'-distance tversky -ensemble 1',\n",
    "                f'-distance euclidean -ensemble 1', \n",
    "                f'-distance cosine -ensemble 1'\n",
    "                ]\n",
    "\n",
    "\n",
    "########################### Train and get the prediction ###############################\n",
    "preds_test = []\n",
    "preds_val = []\n",
    "for arg_str in arg_strs_dist:\n",
    "    args = TopoRegArgs(f'{arg_str}')\n",
    "    # set random seed\n",
    "    set_seed(args.seed)\n",
    "    mdl, pred_test, pred_val, train_time, test_time = TopoReg(desc, target, train_idx, test_idx, val_idx, args)\n",
    "    # Stack the predicted responses for test and validation sets\n",
    "    preds_test.append(pred_test)\n",
    "    preds_val.append(pred_val)\n",
    "\n",
    "########################### Combine the results ###############################\n",
    "# ensemble\n",
    "pred_test = np.array(preds_test).mean(axis=0)\n",
    "# evaluation\n",
    "print('Performance of ensemble predictions:')\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)\n",
    "\n",
    "\n",
    "# stacking the results\n",
    "pred_test, train_time, test_time = stack_models(preds_val, preds_test, target, val_idx)\n",
    "# evaluation\n",
    "print('Performance of stacking predictions:')\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71afa3fa",
   "metadata": {},
   "source": [
    "### Different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07a3763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of ensemble predictions:\n",
      "Spearman: 0.8250608014159587\n",
      "R2: 0.8581550874070674\n",
      "RMSE: 0.6984434917870883\n",
      "NRMSE: 0.3766230377883603\n",
      "Performance of stacking predictions:\n",
      "Spearman: 0.8627928556748934\n",
      "R2: 0.9018851154434258\n",
      "RMSE: 0.5808872463801671\n",
      "NRMSE: 0.31323295573195087\n"
     ]
    }
   ],
   "source": [
    "from extendtr.utils.utils import stack_models\n",
    "# load the descriptor - the indices of desc will be used later\n",
    "desc = pd.read_parquet(f'./SampleDatasets/CHEMBL278/data_ECFP4.parquet', engine='fastparquet').astype('bool')\n",
    "# define arg strs for different anchor selection methods\n",
    "arg_strs_mdls = [f'-ensemble 1', \n",
    "                f'-model LR_L1 -ensemble 1',\n",
    "                f'-model RF -ensemble 1', \n",
    "                f'-model ANN -ensemble 1'\n",
    "                ]\n",
    "\n",
    "\n",
    "########################### Train and get the prediction ###############################\n",
    "preds_test = []\n",
    "preds_val = []\n",
    "for arg_str in arg_strs_dist:\n",
    "    args = TopoRegArgs(f'{arg_str}')\n",
    "    # set random seed\n",
    "    set_seed(args.seed)\n",
    "    mdl, pred_test, pred_val, train_time, test_time = TopoReg(desc, target, train_idx, test_idx, val_idx, args)\n",
    "    # Stack the predicted responses for test and validation sets\n",
    "    preds_test.append(pred_test)\n",
    "    preds_val.append(pred_val)\n",
    "\n",
    "########################### Combine the results ###############################\n",
    "# ensemble\n",
    "pred_test = np.array(preds_test).mean(axis=0)\n",
    "# evaluation\n",
    "print('Performance of ensemble predictions:')\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)\n",
    "\n",
    "\n",
    "# stacking the results\n",
    "pred_test, train_time, test_time = stack_models(preds_val, preds_test, target, val_idx)\n",
    "# evaluation\n",
    "print('Performance of stacking predictions:')\n",
    "scorr, r2, rmse, nrmse = metric_calc(pred_test, target.loc[test_idx], True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TopoReg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
